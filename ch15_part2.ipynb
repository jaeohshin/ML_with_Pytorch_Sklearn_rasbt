{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMjMvgExvv5hYbWXohU82wx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeohshin/ML_with_Pytorch_Sklearn_rasbt/blob/main/ch15_part2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "3MMZ_78WyHT6"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Image\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "5o7e7mgmyNpL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5fMoonfytdb",
        "outputId": "a0d9c619-00a0-4aed-cb83-767dc82fe127"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: torch>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.3.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.3.0->torchtext) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.3.0->torchtext) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.3.0->torchtext) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install portalocker"
      ],
      "metadata": {
        "id": "JpEytmZ-zPrs",
        "outputId": "71d3eec8-c3b8-4665-9ed2-05b1a0ddde62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (2.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.datasets import IMDB\n",
        "from torch.utils.data.dataset import random_split"
      ],
      "metadata": {
        "id": "PMhSNVW5ySQl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87f0f301-5458-44f6-b52a-723a6ced0cfb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/datasets/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/data/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torchdata"
      ],
      "metadata": {
        "id": "YtCfgMOXzewm",
        "outputId": "94ef8bf3-8055-45cf-fbec-c4a451413243",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchdata in /usr/local/lib/python3.10/dist-packages (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.31.0)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.10/dist-packages (from torchdata) (2.3.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2->torchdata) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2->torchdata) (12.5.82)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchdata) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2->torchdata) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2->torchdata) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: load and create the datasets\n",
        "\n",
        "train_dataset = IMDB(split='train')\n",
        "test_dataset = IMDB(split='test')\n",
        "\n",
        "test_dataset = list(test_dataset)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "train_dataset, valid_dataset = random_split(\n",
        "    list(train_dataset), [20000, 5000])"
      ],
      "metadata": {
        "id": "-13HaFxhyZE_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "828C796lz8ub"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_dataset))\n",
        "print(len(test_dataset))"
      ],
      "metadata": {
        "id": "UfcsqXjIzpGz",
        "outputId": "0066ff34-956a-4605-b2ae-655db06ce5f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20000\n",
            "25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 2: find unique tokens (words)\n",
        "import re\n",
        "from collections import Counter, OrderedDict\n",
        "\n",
        "token_counts = Counter()\n",
        "\n",
        "def tokenizer(text):\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text.lower())\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n",
        "        ' '.join(emoticons).replace('-', '')\n",
        "    tokenized = text.split()\n",
        "    return tokenized\n",
        "\n",
        "for label, line in train_dataset:\n",
        "    tokens = tokenizer(line)\n",
        "    token_counts.update(tokens)\n",
        "\n",
        "print('Vocab-size:', len(token_counts))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8orUbK_yoOA",
        "outputId": "8c48d495-347b-4c3f-c6d2-73a26503c81c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab-size: 69023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 3: encoding each unique token into integers\n",
        "from torchtext.vocab import vocab\n",
        "\n",
        "sorted_by_freq_tuples = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "ordered_dict = OrderedDict(sorted_by_freq_tuples)\n",
        "\n",
        "vocab = vocab(ordered_dict)\n",
        "\n",
        "vocab.insert_token(\"<pad>\", 0)\n",
        "vocab.insert_token(\"<unk>\", 1)\n",
        "vocab.set_default_index(1)\n",
        "\n",
        "print([vocab[token] for token in ['this', 'is', 'an', 'example']])\n",
        "print([vocab[token] for token in ['I', 'am', 'a', 'boy']])"
      ],
      "metadata": {
        "id": "Z31ASA0R0LL2",
        "outputId": "202c5223-c145-465b-ee47-fe42ef61f901",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
            "/usr/local/lib/python3.10/dist-packages/torchtext/utils.py:4: UserWarning: \n",
            "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
            "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
            "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[11, 7, 35, 457]\n",
            "[1, 244, 4, 408]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not torch.cuda.is_available():\n",
        "    print(\"Warning: this code may be very slow on CPU\")"
      ],
      "metadata": {
        "id": "ssFMixUe2Jku"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext"
      ],
      "metadata": {
        "id": "HwoSkQks5-22"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 3-A: define the functions for transformation\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "\n",
        "from torchtext import __version__ as torchtext_version\n",
        "from pkg_resources import parse_version\n",
        "\n",
        "if parse_version(torchtext.__version__) > parse_version(\"0.10\"):\n",
        "    label_pipeline = lambda x: 1. if x == 2 else 0.         # 1 ~ 부정 리뷰, 2 ~ 긍정 리뷰\n",
        "else:\n",
        "    label_pipeline = lambda x: 1. if x == 'pos' else 0.\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, lengths = [], [], []\n",
        "    for _label, _text in batch:\n",
        "        label_list.append(label_pipeline(_label)) ## Either 1 or 0\n",
        "        processed_text = torch.tensor(text_pipeline(_text),  ##text --> torken\n",
        "                                      dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "        lengths.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list)\n",
        "    lengths = torch.tensor(lengths)\n",
        "    padded_text_list = nn.utils.rnn.pad_sequence(\n",
        "        text_list, batch_first=True)\n",
        "    return padded_text_list.to(device), label_list.to(device), lengths.to(device)\n"
      ],
      "metadata": {
        "id": "nqHAvDxT3GhQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Take a small batch\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "dataloader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_batch)\n",
        "text_batch, label_batch, length_batch = next(iter(dataloader))\n",
        "print(text_batch)\n",
        "print(label_batch)\n",
        "print(length_batch)\n",
        "print(text_batch.shape)"
      ],
      "metadata": {
        "id": "aGLaJTEq5uzi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "076532c7-d0f8-401a-abfd-7e31d6430a9a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[   35,  1739,     7,   449,   721,     6,   301,     4,   787,     9,\n",
            "             4,    18,    44,     2,  1705,  2460,   186,    25,     7,    24,\n",
            "           100,  1874,  1739,    25,     7, 34415,  3568,  1103,  7517,   787,\n",
            "             5,     2,  4991, 12401,    36,     7,   148,   111,   939,     6,\n",
            "         11598,     2,   172,   135,    62,    25,  3199,  1602,     3,   928,\n",
            "          1500,     9,     6,  4601,     2,   155,    36,    14,   274,     4,\n",
            "         42945,     9,  4991,     3,    14, 10296,    34,  3568,     8,    51,\n",
            "           148,    30,     2,    58,    16,    11,  1893,   125,     6,   420,\n",
            "          1214,    27, 14542,   940,    11,     7,    29,   951,    18,    17,\n",
            "         15994,   459,    34,  2480, 15211,  3713,     2,   840,  3200,     9,\n",
            "          3568,    13,   107,     9,   175,    94,    25,    51, 10297,  1796,\n",
            "            27,   712,    16,     2,   220,    17,     4,    54,   722,   238,\n",
            "           395,     2,   787,    32,    27,  5236,     3,    32,    27,  7252,\n",
            "          5118,  2461,  6390,     4,  2873,  1495,    15,     2,  1054,  2874,\n",
            "           155,     3,  7015,     7,   409,     9,    41,   220,    17,    41,\n",
            "           390,     3,  3925,   807,    37,    74,  2858,    15, 10297,   115,\n",
            "            31,   189,  3506,   667,   163,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [  216,   175,   724,     5,    11,    18,    10,   226,   110,    14,\n",
            "           182,    78,     8,    13,    24,   182,    78,     8,    13,   166,\n",
            "           182,    50,   150,    24,    85,     2,  4031,  5935,   107,    96,\n",
            "            28,  1867,   602,    19,    52,   162,    21,  1698,     8,     6,\n",
            "          1181,   367,     2,   351,    10,   140,   419,     4,   333,     5,\n",
            "          6022,  7136,  5055,  1209, 10892,    32,   219,     9,     2,   405,\n",
            "          1413,    13,  4031,    13,  1099,     7,    85,    19,     2,    20,\n",
            "          1018,     4,    85,   565,    34,    24,   807,    55,     5,    68,\n",
            "           658,    10,   507,     8,     4,   668,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0],\n",
            "        [   10,   121,    24,    28,    98,    74,   589,     9,   149,     2,\n",
            "          7372,  3030, 14543,  1012,   520,     2,   985,  2327,     5, 16847,\n",
            "          5479,    19,    25,    67,    76,  3478,    38,     2,  7372,     3,\n",
            "            25,    67,    76,  2951,    34,    35, 10893,   155,   449, 29495,\n",
            "         23725,    10,    67,     2,   554,    12, 14543,    67,    91,     4,\n",
            "            50,    20,    19,     8,    67,    24,  4228,     2,  2142,    37,\n",
            "            33,  3478,    87,     3,  2564,   160,   155,    11,   634,   126,\n",
            "            24,   158,    72,   286,    13,   373,     2,  4804,    19,     2,\n",
            "          7372,  6794,     6,    30,   128,    73,    48,    10,   886,     8,\n",
            "            13,    24,     4,    85,    20,    19,     8,    13,    35,   218,\n",
            "             3,   428,   710,     2,   107,   936,     7,    54,    72,   223,\n",
            "             3,    10,    96,   122,     2,   103,    54,    72,    82,     2,\n",
            "           658,   202,     2,   106,   293,   103,     7,  1193,     3,  3031,\n",
            "           708,  5760,     3,  2918,  3991,   706,  3327,   349,   148,   286,\n",
            "            13,   139,     6,     2,  1501,   750,    29,  1407,    62,    65,\n",
            "          2612,    71,    40,    14,     4,   547,     9,    62,     8,  7943,\n",
            "            71,    14,     2,  5687,     5,  4868,  3111,     6,   205,     2,\n",
            "            18,    55,  2075,     3,   403,    12,  3111,   231,    45,     5,\n",
            "           271,     3,    68,  1400,     7,  9774,   932,    10,   102,     2,\n",
            "            20,   143,    28,    76,    55,  3810,     9,  2723,     5,    12,\n",
            "            10,   379,     2,  7372,    15,     4,    50,   710,     8,    13,\n",
            "            24,   887,    32,    31,    19,     8,    13,   428],\n",
            "        [18923,     7,     4,  4753,  1669,    12,  3019,     6,     4, 13906,\n",
            "           502,    40,    25,    77,  1588,     9,   115,     6, 21713,     2,\n",
            "            90,   305,   237,     9,   502,    33,    77,   376,     4, 16848,\n",
            "           847,    62,    77,   131,     9,     2,  1580,   338,     5, 18923,\n",
            "            32,     2,  1980,    49,   157,   306, 21713,    46,   981,     6,\n",
            "         10298,     2, 18924,   125,     9,   502,     3,   453,     4,  1852,\n",
            "           630,   407,  3407,    34,   277,    29,   242,     2, 20200,     5,\n",
            "         18923,    77,    95,    41,  1833,     6,  2105,    56,     3,   495,\n",
            "           214,   528,     2,  3479,     2,   112,     7,   181,  1813,     3,\n",
            "           597,     5,     2,   156,   294,     4,   543,   173,     9,  1562,\n",
            "           289, 10038,     5,     2,    20,    26,   841,  1392,    62,   130,\n",
            "           111,    72,   832,    26,   181, 12402,    15,    69,   183,     6,\n",
            "            66,    55,   936,     5,     2,    63,     8,     7,    43,     4,\n",
            "            78, 23726, 15995,    13,    20,    17,   800,     5,   392,    59,\n",
            "          3992,     3,   371,   103,  2596,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
            "       device='cuda:0')\n",
            "tensor([1., 1., 1., 0.], device='cuda:0')\n",
            "tensor([165,  86, 218, 145], device='cuda:0')\n",
            "torch.Size([4, 218])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Step 4: batching the datasets\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_dl = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                      shuffle=True, collate_fn=collate_batch)\n",
        "valid_dl = DataLoader(valid_dataset, batch_size=batch_size,\n",
        "                      shuffle=False, collate_fn=collate_batch)\n",
        "test_dl = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                      shuffle=False, collate_fn=collate_batch)"
      ],
      "metadata": {
        "id": "MCpLP42SSOqU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## I don't understand this\n",
        "embedding = nn.Embedding(\n",
        "    num_embeddings=10,\n",
        "    embedding_dim=3,\n",
        "    padding_idx=0\n",
        ")"
      ],
      "metadata": {
        "id": "FnYDTay6UZZP"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oD0GT9tM_Imq",
        "outputId": "d20ca7e9-57f3-488c-dde8-06e377596992"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(10, 3, padding_idx=0)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoded_input = torch.LongTensor([1, 2, 4, 5])\n",
        "print(embedding(text_encoded_input))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lnbhXCHh_KCT",
        "outputId": "7f52100b-4d47-4178-cdf6-57ec6fca94e8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.0233, -0.5962, -1.0055],\n",
            "        [-0.2106, -0.0075, -1.7869],\n",
            "        [-0.9962, -0.8313,  1.3075],\n",
            "        [-1.1628,  0.1196, -0.1631]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## RNN model\n",
        "\n",
        "## Fully connected NN with one hidden layer\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.RNN(input_size,\n",
        "                          hidden_size,\n",
        "                          num_layers=2,\n",
        "                          batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, hidden = self.rnn(x)\n",
        "        out = hidden[-1, :, :]\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "model = RNN(64, 32)\n",
        "print(model)\n",
        "\n",
        "print(torch.randn(5, 3, 64))\n",
        "model(torch.randn(5, 3, 64))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeQ2P60A_R0r",
        "outputId": "b2ba3669-2e1e-41d5-8463-60072c17e18e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (rnn): RNN(64, 32, num_layers=2, batch_first=True)\n",
            "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
            ")\n",
            "tensor([[[-4.7983e-01, -7.9163e-01,  6.2671e-01, -1.7690e+00,  3.4495e-01,\n",
            "           8.1368e-01, -7.4264e-01, -1.3547e+00,  6.1753e-01,  3.4495e-01,\n",
            "          -1.7448e+00, -9.9707e-01, -2.6254e-01,  2.0749e+00,  1.4882e+00,\n",
            "           5.1290e-01,  9.3440e-01,  2.5334e-01,  1.7863e-01,  2.2715e-01,\n",
            "           9.1273e-01, -8.8465e-01,  5.7100e-01, -5.2118e-01, -1.2458e+00,\n",
            "          -1.4760e+00, -1.6932e+00, -6.7292e-01,  4.7573e-01,  1.2609e+00,\n",
            "          -5.7301e-01,  1.9036e+00, -7.7445e-01,  7.5189e-01, -1.5651e-01,\n",
            "          -7.0961e-01, -9.3730e-01, -8.0417e-01,  6.6968e-01, -8.0389e-01,\n",
            "           9.8054e-01, -4.8534e-01,  1.1331e+00, -7.5181e-01, -5.4427e-02,\n",
            "          -1.0632e+00, -2.4394e+00, -8.8242e-01, -1.7347e-01,  1.5764e+00,\n",
            "          -6.7883e-01,  9.7769e-01,  1.1637e+00,  3.7583e-02,  6.1172e-01,\n",
            "           3.8425e-01, -1.1042e+00, -1.4609e-01,  1.1131e+00,  3.3578e-01,\n",
            "          -7.1988e-01,  7.5123e-01, -1.0954e+00, -5.5383e-01],\n",
            "         [-3.9748e-02,  1.0913e+00, -1.0608e-01, -1.0892e+00,  4.9146e-01,\n",
            "           3.7142e-01, -3.5400e-01, -1.0589e+00, -8.8404e-01, -4.9676e-01,\n",
            "           8.1779e-01, -1.9539e+00,  2.4540e-01, -1.2184e+00,  6.0713e-01,\n",
            "          -1.8347e+00,  1.4957e+00, -4.0059e-01, -1.4889e+00, -6.9991e-01,\n",
            "           1.5363e+00, -1.5508e-01, -5.8427e-01, -4.8558e-01, -6.9227e-01,\n",
            "          -5.0220e-01, -9.0562e-01,  4.5906e-01,  1.8817e-02,  8.8016e-01,\n",
            "           1.0655e+00, -1.1683e+00,  1.1130e+00, -8.6912e-01,  5.9204e-01,\n",
            "           2.3257e-03,  2.7230e-01,  9.1493e-02, -1.3337e+00,  2.1974e+00,\n",
            "           8.6491e-02, -1.1758e+00, -4.4722e-01,  6.7938e-02, -1.4916e+00,\n",
            "          -1.7118e+00, -1.0540e+00,  3.0244e-01, -1.6142e+00, -2.2879e-01,\n",
            "           1.0401e+00,  5.7286e-01, -1.3634e+00, -4.3596e-01,  2.4391e-01,\n",
            "          -1.4638e+00,  2.1400e-01,  2.4290e+00,  2.1772e-01,  1.4407e+00,\n",
            "          -1.5487e+00,  2.8438e-01,  1.0647e-01,  4.8156e-01],\n",
            "         [ 9.9839e-01, -2.1297e-02,  1.2484e+00, -1.0263e+00, -6.4871e-01,\n",
            "           1.9485e-01, -2.5889e-02, -9.0379e-02, -5.3930e-01,  2.5993e-01,\n",
            "          -1.2876e+00,  3.8027e+00, -1.8544e+00, -2.5640e-01,  6.5328e-01,\n",
            "          -4.7249e-01,  5.9323e-01,  1.0186e+00,  7.5483e-01, -8.8263e-02,\n",
            "          -1.3078e+00, -3.1163e-01, -6.5905e-01,  5.3885e-01, -1.5427e+00,\n",
            "          -9.3279e-01,  4.1886e-01,  4.8685e-01,  3.7937e-01,  1.9154e-02,\n",
            "           6.1808e-01,  1.2037e+00,  9.2341e-01,  4.6968e-01,  3.5608e-01,\n",
            "           1.4399e+00, -4.6349e-01,  3.8159e-01,  1.2468e+00,  7.6273e-01,\n",
            "          -3.6421e-01,  8.8743e-01,  5.5798e-01, -3.0944e-01, -2.9927e-01,\n",
            "          -5.1018e-02, -4.8145e-01, -1.8018e-02,  2.0231e+00, -1.2475e+00,\n",
            "          -6.5382e-01, -1.4035e-01, -2.4679e-01,  1.0288e+00,  1.2715e+00,\n",
            "           1.1249e+00,  4.6202e-01, -1.2690e+00, -1.1892e-01, -9.3235e-01,\n",
            "           1.0619e+00, -5.7093e-01,  1.6775e+00, -9.1810e-01]],\n",
            "\n",
            "        [[ 2.4087e+00,  3.7212e-01,  5.4021e-01,  2.7674e-01,  4.1698e-01,\n",
            "          -7.4126e-01,  6.6840e-01,  3.3362e-01, -3.9492e-01, -2.4390e-01,\n",
            "          -2.4923e-01, -1.8581e+00,  2.0162e-01, -8.4637e-01, -1.8241e-01,\n",
            "          -1.6628e+00, -6.1583e-01,  2.7401e-02, -9.1845e-01, -3.1513e-01,\n",
            "          -5.4031e-02,  5.5821e-01,  4.7974e-01,  8.9825e-01, -1.8796e-01,\n",
            "           1.5841e+00,  1.0233e+00,  5.6355e-01, -3.5256e-01,  4.2628e-01,\n",
            "          -7.2043e-02, -6.5485e-02, -7.8821e-01,  1.0097e+00,  2.5077e-01,\n",
            "           4.0177e-01,  1.6462e+00, -1.9608e-01, -2.0892e-01, -8.7863e-01,\n",
            "           2.1473e+00, -1.8066e+00,  1.0527e+00,  6.3690e-01,  2.0833e+00,\n",
            "           1.8356e-02,  9.4674e-03,  2.0072e+00,  1.3511e+00,  8.3507e-01,\n",
            "          -5.7974e-01,  1.7379e+00,  8.3746e-02,  7.0517e-01,  8.9416e-01,\n",
            "           3.5158e-01, -1.3960e-01,  1.5718e+00, -5.4485e-01,  2.7967e-01,\n",
            "          -8.6063e-01, -1.2625e+00,  1.3979e-01,  6.3771e-01],\n",
            "         [-1.0836e-01, -1.2965e+00,  7.2178e-02,  1.5534e+00,  1.7707e+00,\n",
            "          -9.6501e-01,  5.2504e-01, -1.3892e-01, -1.6465e-01,  5.1957e-01,\n",
            "          -2.9037e-01, -1.8342e-02,  2.0613e+00,  4.7715e-01,  1.5680e+00,\n",
            "          -4.4514e-01,  2.1387e+00, -6.9078e-01,  2.2985e+00, -1.8984e+00,\n",
            "           5.3780e-01,  1.6787e+00,  5.5186e-01, -1.3614e-01,  1.1680e+00,\n",
            "          -7.9168e-01,  7.8609e-01, -3.3425e-01,  9.2061e-01, -3.8132e-01,\n",
            "           2.1789e+00,  1.2308e+00,  1.1898e-02, -6.0462e-01,  2.7854e+00,\n",
            "          -2.6920e-01, -1.0609e+00,  6.6863e-01,  9.5064e-01,  1.1404e+00,\n",
            "          -1.8795e+00,  1.5130e-01, -1.1346e+00, -1.2885e+00,  1.3222e+00,\n",
            "           5.9542e-02, -6.4180e-01, -1.1479e+00, -5.6780e-01, -9.8014e-01,\n",
            "           3.2173e-01, -2.0487e+00,  6.2733e-01, -5.5570e-01,  1.5062e-01,\n",
            "           1.0637e+00,  4.4200e-01, -7.2341e-01, -1.5938e+00,  1.6366e-01,\n",
            "          -5.7716e-01,  1.5001e+00,  6.7981e-01, -6.5697e-01],\n",
            "         [-1.1221e+00,  9.1716e-01,  1.0659e+00, -1.3906e+00, -1.0463e+00,\n",
            "          -5.5655e-01, -6.6404e-01, -7.4107e-01, -4.2230e-01,  2.3038e-01,\n",
            "          -9.1057e-01,  4.5608e-03,  2.2700e-03, -4.9197e-01, -3.5232e-01,\n",
            "          -7.0516e-01, -5.9668e-01,  1.0327e+00,  5.9021e-01,  1.3539e+00,\n",
            "           2.3891e+00,  1.7375e+00, -1.5341e-01, -1.8514e+00, -1.0123e+00,\n",
            "           3.1878e-01, -1.3093e-01, -9.1004e-01, -8.6431e-01,  6.8917e-01,\n",
            "           2.0398e-01, -4.5387e-01, -5.3996e-01,  1.1330e-01,  2.7725e-01,\n",
            "           1.3704e+00, -5.7366e-01, -1.0653e+00,  1.9249e+00, -9.3628e-01,\n",
            "          -4.7725e-01,  1.1411e+00,  9.7908e-01, -5.2158e-01, -2.0939e-01,\n",
            "           2.4465e-01,  1.1430e+00,  1.6321e+00,  3.1205e-02, -1.5502e+00,\n",
            "           1.1290e+00, -9.7958e-01, -1.4674e+00,  4.7537e-01, -1.3177e+00,\n",
            "          -2.0036e+00,  1.9925e-01,  4.3617e-01, -5.8604e-01,  2.5783e-01,\n",
            "          -1.2422e-01, -1.2387e+00, -3.7856e-01,  4.9693e-01]],\n",
            "\n",
            "        [[ 1.5681e+00, -3.9614e-01,  1.4424e-02,  1.0389e+00, -9.6029e-01,\n",
            "           1.1592e-01, -1.0864e+00,  8.9502e-01,  6.8373e-01, -1.4452e+00,\n",
            "          -1.6855e-01,  1.1338e+00,  3.4197e-01, -1.3247e+00, -5.8319e-01,\n",
            "          -1.6608e+00, -1.3525e+00,  1.2078e-01, -3.0706e+00,  9.7947e-01,\n",
            "           9.8894e-01,  4.7585e-01, -4.2413e-01, -9.8494e-01, -1.6832e+00,\n",
            "          -2.3033e+00, -7.2421e-01, -2.6495e+00, -3.7556e-01, -8.7600e-01,\n",
            "          -3.4192e-01,  1.4201e-01,  5.0285e-01,  7.0525e-01, -7.3662e-01,\n",
            "          -5.5156e-01,  9.4024e-01,  2.3200e+00, -4.3308e-01,  1.3208e+00,\n",
            "          -3.4530e-01,  1.6229e+00,  3.5538e-01, -1.0201e+00, -1.8698e+00,\n",
            "           2.4121e+00,  9.9473e-01, -1.4550e+00,  1.0490e+00,  5.8474e-01,\n",
            "          -1.7653e+00,  8.2271e-02,  6.6622e-01,  1.2693e+00,  1.3017e+00,\n",
            "          -5.3680e-01,  1.8036e+00, -5.7457e-01, -2.7733e-01, -1.2495e+00,\n",
            "           4.5590e-01,  9.9641e-01, -1.6358e+00, -3.9397e-02],\n",
            "         [ 7.8814e-01, -3.1185e-01, -4.7158e-01,  1.0008e+00,  2.7717e-02,\n",
            "           1.4864e+00, -3.7932e-01, -4.8731e-01, -9.4639e-01, -7.8076e-01,\n",
            "          -1.7243e+00,  9.6467e-01, -1.7401e-01,  7.5635e-01,  1.2253e+00,\n",
            "          -6.9498e-01, -7.3682e-01,  4.9731e-01, -4.7207e-01, -7.5152e-01,\n",
            "          -1.9064e+00, -9.5415e-01, -3.8395e-01,  1.8689e+00,  3.8960e-03,\n",
            "           1.7015e+00,  1.6748e+00, -1.1034e+00,  5.6516e-01,  6.3025e-01,\n",
            "          -8.2909e-01,  3.4272e-01, -6.4993e-01, -8.6748e-02,  1.1141e+00,\n",
            "          -7.5839e-01, -6.5518e-02,  1.2533e+00, -1.2912e+00,  2.3245e-01,\n",
            "          -5.8478e-01,  1.9108e-01, -4.9082e-01,  1.2234e+00, -1.3258e+00,\n",
            "          -1.7900e-01, -5.9711e-01,  1.3147e+00,  7.7160e-02, -1.6202e-02,\n",
            "           4.1519e-01, -1.2441e+00, -1.3966e+00,  6.4460e-01,  1.1298e+00,\n",
            "           3.5218e-02,  9.7188e-01,  3.1362e-01,  4.0499e-01,  2.3594e-01,\n",
            "          -1.7688e+00, -8.1041e-01,  5.5193e-02, -1.2400e+00],\n",
            "         [ 3.8309e-01,  7.4836e-01,  3.7447e-01,  6.0836e-01,  4.2287e-01,\n",
            "           1.0183e+00,  3.0330e-01,  6.4081e-01, -1.7899e-01,  1.4646e+00,\n",
            "          -4.3717e-01, -1.6058e-01,  4.9220e-02,  2.0241e+00, -2.9207e-01,\n",
            "           1.8069e+00, -1.8670e+00, -6.7396e-01, -4.1374e-02,  1.5627e-01,\n",
            "           1.2657e+00,  1.9662e-01,  5.4235e-01,  1.1338e+00,  2.0529e-01,\n",
            "           1.3807e-01,  1.0474e+00, -6.0849e-01, -3.3669e-01, -6.2949e-01,\n",
            "           4.5053e-01,  7.1339e-01, -3.4751e-01, -1.1731e+00, -5.8827e-01,\n",
            "           8.3890e-01, -1.3685e+00,  1.7561e+00,  1.1744e+00,  8.4713e-01,\n",
            "           5.2931e-01,  3.4143e-01,  9.5546e-01,  9.9356e-01,  5.5106e-01,\n",
            "           3.8945e-01, -1.1740e+00, -1.1235e+00, -8.5919e-01,  3.1177e-01,\n",
            "           2.1418e-02, -6.0900e-01, -1.9654e+00, -2.2240e+00,  7.1068e-01,\n",
            "           1.4887e+00,  8.7821e-01,  5.1910e-01,  9.2872e-01,  1.2546e+00,\n",
            "          -8.6016e-01, -8.5775e-02, -1.2760e+00, -6.0576e-01]],\n",
            "\n",
            "        [[ 6.0076e-03,  1.0617e+00,  3.4948e-01,  8.4948e-01, -2.3620e-01,\n",
            "           5.8347e-02,  8.0982e-01,  5.1901e-01,  5.6411e-01, -1.1628e+00,\n",
            "           8.0876e-01,  1.4118e+00,  4.4055e-02, -9.2098e-01,  1.5412e+00,\n",
            "          -8.4960e-01,  1.5731e-01,  1.9317e+00, -5.1714e-01,  5.3215e-01,\n",
            "           1.0493e-01, -4.0299e-01, -2.8451e+00,  1.0732e+00,  8.6502e-02,\n",
            "           7.8049e-01,  8.7550e-02, -1.1018e+00, -5.8921e-01, -5.6089e-02,\n",
            "          -1.2283e+00,  1.1443e+00, -1.7914e+00,  1.4559e-01,  3.4841e-01,\n",
            "           5.1879e-01, -9.5932e-01, -6.3777e-01, -8.4733e-01, -2.3947e+00,\n",
            "          -2.4343e-01,  1.0776e+00, -2.8892e-01, -1.1901e-01, -3.0230e-01,\n",
            "           5.0339e-02, -3.3096e-01,  1.7578e+00, -7.2915e-01,  8.0151e-01,\n",
            "          -1.6327e+00, -1.0415e+00, -9.3865e-01, -1.0576e-02,  7.1830e-01,\n",
            "          -8.3925e-01, -1.1249e+00, -1.2209e+00, -1.2377e+00,  1.2545e+00,\n",
            "          -9.7096e-01,  1.1140e+00, -1.9379e+00,  1.3125e+00],\n",
            "         [-9.8011e-01,  1.4174e+00,  5.1727e-01, -1.9137e-01, -7.1153e-01,\n",
            "          -2.9120e-02,  5.9093e-01,  9.2752e-01, -9.7121e-01, -1.8757e-01,\n",
            "           3.0848e-02, -3.6834e-01,  1.7903e+00, -1.0964e-01, -6.6095e-01,\n",
            "          -1.4895e+00, -5.5116e-02,  5.6399e-01,  1.4076e+00, -1.8125e-01,\n",
            "          -3.2884e-01, -1.8100e+00,  6.4441e-01,  7.3086e-01,  7.5063e-02,\n",
            "           6.7985e-01,  1.3466e+00,  1.5136e+00,  2.9716e-01, -1.3718e-01,\n",
            "          -1.8296e+00, -2.4653e-01, -6.9965e-01, -5.9383e-01, -8.1055e-01,\n",
            "          -1.5753e-01, -1.9555e-01, -1.5301e+00, -1.1289e+00, -1.6374e-01,\n",
            "           1.0623e+00,  1.6828e+00, -1.5945e+00,  1.3405e+00, -7.5679e-01,\n",
            "           2.6939e-01,  6.7155e-01, -4.3709e-01,  7.2660e-01,  1.4487e+00,\n",
            "          -1.1032e+00, -9.7525e-01, -5.5413e-01,  1.2726e+00, -9.9705e-01,\n",
            "           1.9083e-02, -1.6572e+00,  1.5110e-01, -7.3747e-01,  5.6220e-01,\n",
            "          -8.6638e-01, -1.0525e+00, -1.4338e-01,  2.0551e+00],\n",
            "         [-1.0292e+00, -4.4928e-01,  1.2412e+00, -1.2158e+00,  6.8796e-01,\n",
            "          -4.0841e-01, -1.0639e+00, -1.8271e+00,  1.5745e-01, -6.8454e-01,\n",
            "           1.2012e+00, -1.2084e+00, -9.8938e-01, -7.9792e-02,  5.1142e-01,\n",
            "          -2.3278e+00, -9.7511e-02, -6.6435e-01,  9.0652e-01, -3.2720e-02,\n",
            "          -8.6749e-01,  4.4221e-01,  1.6503e-01,  8.3703e-02, -3.1639e-01,\n",
            "          -9.7102e-01,  2.1408e-01,  2.7311e+00,  5.7635e-01, -2.0643e+00,\n",
            "          -8.9905e-01,  7.6958e-01,  1.1816e-01,  1.5396e+00, -2.0150e-01,\n",
            "           9.2584e-01, -3.8343e-03,  1.0841e+00, -4.9183e-01,  1.9394e+00,\n",
            "           2.9503e-01,  1.7959e+00,  8.4564e-01, -1.3266e+00,  3.6709e-01,\n",
            "           8.1094e-02,  1.3119e+00, -8.1122e-01,  8.6427e-03, -2.7226e-01,\n",
            "           6.2858e-01,  7.9659e-01,  1.5408e+00,  5.6242e-01,  1.6617e+00,\n",
            "          -3.1262e-01, -2.2804e-01, -1.3907e+00, -2.1337e+00, -1.6208e+00,\n",
            "          -1.9058e+00, -7.2572e-01,  9.4733e-01, -2.4144e-01]],\n",
            "\n",
            "        [[ 3.1999e-01, -1.0326e+00,  8.3477e-01, -5.0569e-01,  4.6326e-01,\n",
            "           1.2100e+00, -6.4521e-01,  2.9082e-01, -1.4465e-01, -1.0494e+00,\n",
            "          -9.0461e-02, -6.0069e-01, -1.0736e+00,  4.6318e-01, -6.1827e-03,\n",
            "          -5.1574e-02,  9.1904e-02, -3.0504e-02,  1.1990e+00,  2.7843e-01,\n",
            "          -6.1230e-01,  1.6596e-02, -1.1996e+00,  4.0558e-01, -1.1594e+00,\n",
            "           8.4105e-01,  5.8655e-01,  4.7962e-01, -1.3364e-01, -7.0800e-01,\n",
            "           6.5681e-01,  4.9341e-02,  5.0255e-02, -2.4878e+00,  6.5238e-01,\n",
            "          -3.4882e-01,  2.7299e-01,  4.5391e-01, -3.4483e-02, -9.0998e-01,\n",
            "           1.1727e+00, -6.4185e-01, -2.9952e-01, -1.2582e+00, -1.3980e+00,\n",
            "          -4.9943e-01,  5.1260e-01, -1.3689e+00,  2.7593e-01, -5.0801e-01,\n",
            "          -1.0988e+00,  6.6404e-01, -7.3698e-02,  9.9922e-02,  8.2710e-01,\n",
            "          -5.7422e-01, -2.0773e+00,  1.8778e-01,  9.2857e-01,  5.7172e-01,\n",
            "          -3.9048e-01,  4.3361e-01, -1.0054e+00,  6.0661e-02],\n",
            "         [ 3.6383e-01,  1.6510e-01,  1.7564e+00,  3.4729e-01, -6.8670e-01,\n",
            "          -8.1138e-01, -4.4982e-01,  1.0777e+00,  4.1068e-01, -2.0572e+00,\n",
            "           1.0906e+00,  1.1871e-01, -5.6796e-02,  4.5861e-02, -3.6010e-01,\n",
            "           6.6626e-01,  2.0555e+00,  4.0238e-01,  1.8500e+00, -3.1449e-01,\n",
            "          -1.0525e+00, -4.2807e-02, -7.0896e-01,  4.2107e-01, -1.2450e+00,\n",
            "           1.9500e-01, -9.0100e-01,  2.4731e+00, -9.3908e-01, -1.3779e+00,\n",
            "          -1.0940e-01,  1.3960e+00, -1.8875e-01,  5.3792e-01,  7.0837e-01,\n",
            "          -1.5471e-01,  2.3131e-01,  5.7998e-01, -1.3723e-01, -5.1614e-01,\n",
            "          -6.4726e-01, -1.0953e+00, -8.8859e-02, -4.7989e-01, -8.2376e-02,\n",
            "           3.0707e+00, -2.6743e-01, -2.5474e-01, -3.0057e-01, -4.4892e-02,\n",
            "           3.1997e-01, -1.6532e+00, -1.4624e+00,  7.1048e-01,  5.4654e-01,\n",
            "          -1.7545e+00, -7.2142e-02,  2.3937e+00, -1.4490e+00,  1.8539e-01,\n",
            "          -9.1673e-01, -1.2842e+00, -5.0007e-01,  4.3525e-01],\n",
            "         [ 3.0719e-01,  1.0226e+00, -3.7609e-01,  6.0577e-02,  1.8271e+00,\n",
            "           5.6580e-01, -7.5871e-01, -8.1847e-01, -1.1653e+00, -6.9140e-01,\n",
            "           1.5998e+00, -1.5271e+00, -1.9790e-01, -7.3388e-01, -2.0948e-01,\n",
            "           5.4616e-01,  1.7436e+00, -1.2712e+00, -8.2214e-01, -1.5186e-02,\n",
            "           1.1436e-01,  2.1073e-01,  1.0361e+00, -7.5969e-01,  2.4496e+00,\n",
            "           7.7019e-01,  2.7311e-01,  1.3734e+00,  6.9657e-01, -6.9313e-01,\n",
            "          -2.0210e+00,  1.5281e+00, -1.0040e-02,  1.4196e+00,  1.9974e-01,\n",
            "           9.4824e-01, -5.0027e-01, -8.3031e-01,  9.2735e-02,  1.3854e+00,\n",
            "           1.9297e-01, -1.4380e+00,  6.1878e-01,  5.8742e-01, -5.5758e-01,\n",
            "           1.7275e-01, -1.8725e+00, -1.6822e+00,  3.1325e-01,  1.3335e+00,\n",
            "           1.3019e+00,  5.7375e-01, -2.8694e-01,  1.2508e+00,  1.3580e-01,\n",
            "          -1.0006e+00,  8.1039e-01,  2.3720e-01, -7.5457e-01,  1.0653e+00,\n",
            "           1.0087e+00, -6.8346e-01,  3.8435e-01,  4.9117e-01]]])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2139],\n",
              "        [0.1112],\n",
              "        [0.4663],\n",
              "        [0.0936],\n",
              "        [0.1062]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size,\n",
        "                                      embed_dim,\n",
        "                                      padding_idx=0)\n",
        "        self.rnn = nn.LSTM(embed_dim, rnn_hidden_size,\n",
        "                           batch_first=True)\n",
        "        self.fc1 = nn.Linear(rnn_hidden_size, fc_hidden_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(fc_hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text, lengths):\n",
        "        out = self.embedding(text)\n",
        "        out = nn.utils.rnn.pack_padded_sequence(out, lengths.cpu().numpy(), enforce_sorted=False,\n",
        "            batch_first=True\n",
        "        )\n",
        "        out, (hidden, cell) = self.rnn(out)\n",
        "        out = hidden[-1, :, :]\n",
        "        out = self.fc1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out= self.sigmoid(out)\n",
        "        return out\n",
        "\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 20\n",
        "rnn_hidden_size = 64\n",
        "fc_hidden_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "model = RNN(vocab_size, embed_dim, rnn_hidden_size, fc_hidden_size)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "0n_bPg0lDzI6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    for text_batch, label_batch, lengths in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        pred = model(text_batch, lengths)[:, 0]\n",
        "        loss = loss_fn(pred, label_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
        "        total_loss += loss.item()*label_batch.size(0)\n",
        "    return total_acc/len(dataloader.dataset), total_loss/len(dataloader.dataset)\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_loss = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for text_batch, label_batch, lengths in dataloader:\n",
        "            pred = model(text_batch, lengths)[:, 0]\n",
        "            loss = loss_fn(pred, label_batch)\n",
        "            total_acc += ((pred>=0.5).float() == label_batch).float().sum().item()\n",
        "            total_loss += loss.item()*label_batch.size(0)\n",
        "    return total_acc/len(list(dataloader.dataset)), total_loss/len(list(dataloader.dataset))\n",
        "\n"
      ],
      "metadata": {
        "id": "e7UUObAnGdwV"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    acc_train, loss_train = train(train_dl)\n",
        "    acc_valid, loss_valid = evaluate(valid_dl)\n",
        "    print(f'에포크 {epoch} 정확도: {acc_train:.4f} 검증 정확도: {acc_valid:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "4EV6Wgq8IA4L",
        "outputId": "b726a46b-eb0c-4d70-bcf0-6ee47cc15fe7"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "RNN.forward() takes 2 positional arguments but 3 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-f1990404f59f>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0macc_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0macc_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_dl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'에포크 {epoch} 정확도: {acc_train:.4f} 검증 정확도: {acc_valid:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-7bd8c565e540>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(dataloader)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1532\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1534\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1539\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1543\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: RNN.forward() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y9M-g3V3IaG1"
      },
      "execution_count": 22,
      "outputs": []
    }
  ]
}