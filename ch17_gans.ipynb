{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQFQ+bwamlp7U+fu3/0i/M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeohshin/ML_with_Pytorch_Sklearn_rasbt/blob/main/ch17_gans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mM8Gq99EI1wE"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '..')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(torch.__version__)\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = \"cpu\""
      ],
      "metadata": {
        "id": "Wr3OtFCh6YBs",
        "outputId": "554fdeb1-7ef6-47f1-bcfa-43e2c1c56d24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.1+cu121\n",
            "GPU Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "jjY2nuvW6pi5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## define a function for the generator:\n",
        "\n",
        "def make_generator_network(\n",
        "        input_size = 20,\n",
        "        num_hidden_layers=1,\n",
        "        num_hidden_units=100,\n",
        "        num_output_units=748):\n",
        "    model = nn.Sequential()\n",
        "    for i in range(num_hidden_layers):\n",
        "        model.add_module(f'fc_g{i}',\n",
        "                        nn.Linear(input_size,\n",
        "                                  num_hidden_units))\n",
        "        model.add_module(f'relu_g{i}',\n",
        "                         nn.LeakyReLU())\n",
        "        input_size = num_hidden_units\n",
        "\n",
        "    model.add_module(f'fc_g{num_hidden_layers}',\n",
        "                     nn.Linear(input_size, num_output_units))\n",
        "    model.add_module('tanh_g', nn.Tanh())\n",
        "    return model\n",
        "\n",
        "## define a function for the discriminator:\n",
        "\n",
        "def make_discriminator_network(\n",
        "        input_size = 748,\n",
        "        num_hidden_layers = 1,\n",
        "        num_hidden_units=100,\n",
        "        num_output_units=1):\n",
        "    model = nn.Sequential()\n",
        "    for i in range(num_hidden_layers):\n",
        "        model.add_module(f'fc_d{i}',\n",
        "                nn.Linear(input_size,\n",
        "                          num_hidden_units, bias=False))\n",
        "        model.add_module(f'relu_d{i}',\n",
        "                         nn.LeakyReLU())\n",
        "        model.add_module('dropout', nn.Dropout(p=0.5))\n",
        "        input_size = num_hidden_units\n",
        "    model.add_module(f'fc_d{num_hidden_layers}',\n",
        "                     nn.Linear(input_size, num_output_units))\n",
        "    model.add_module('sigmoid', nn.Sigmoid())\n",
        "    return model"
      ],
      "metadata": {
        "id": "S2FDh-_R7a2D"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "image_size = (28, 28)\n",
        "z_size = 20\n",
        "\n",
        "gen_hidden_layers = 1\n",
        "gen_hidden_size = 100\n",
        "disc_hidden_layers = 1\n",
        "disc_hidden_size = 100\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "gen_model = make_generator_network(\n",
        "    input_size=z_size,\n",
        "    num_hidden_layers=gen_hidden_layers,\n",
        "    num_hidden_units=gen_hidden_size,\n",
        "    num_output_units=np.prod(image_size))\n",
        "\n",
        "print(gen_model)"
      ],
      "metadata": {
        "id": "3wsl_MG3_w-f",
        "outputId": "9c5adaba-c079-4efd-c20d-90fba498c4ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (fc_g0): Linear(in_features=20, out_features=100, bias=True)\n",
            "  (relu_g0): LeakyReLU(negative_slope=0.01)\n",
            "  (fc_g1): Linear(in_features=100, out_features=784, bias=True)\n",
            "  (tanh_g): Tanh()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "disc_model = make_discriminator_network(\n",
        "    input_size=np.prod(image_size),\n",
        "    num_hidden_layers=disc_hidden_layers,\n",
        "    num_hidden_units=disc_hidden_size)\n",
        "\n",
        "print(disc_model)"
      ],
      "metadata": {
        "id": "JFUrKNUAcJ3k",
        "outputId": "be56187a-bc81-4dd9-c949-d721fdf6670b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (fc_d0): Linear(in_features=784, out_features=100, bias=False)\n",
            "  (relu_d0): LeakyReLU(negative_slope=0.01)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc_d1): Linear(in_features=100, out_features=1, bias=True)\n",
            "  (sigmoid): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "image_path = './'\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5)),\n",
        "])\n",
        "mnist_dataset = torchvision.datasets.MNIST(root=image_path,\n",
        "                                           train=True,\n",
        "                                           transform=transform,\n",
        "                                           download=True)\n",
        "\n",
        "example, label = next(iter(mnist_dataset))\n",
        "print(f'Min: {example.min()} Max: {example.max()}')\n",
        "print(example.shape)"
      ],
      "metadata": {
        "id": "BQFzKi6qcLoV",
        "outputId": "caf2b0d4-606e-4341-fade-7150c7b5f14a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 15913288.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 475569.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4361791.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3044674.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Min: -1.0 Max: 1.0\n",
            "torch.Size([1, 28, 28])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_noise(batch_size, z_size, mode_z):\n",
        "    if mode_z == 'uniform':\n",
        "        input_z = torch.rand(batch_size, z_size)*2 - 1\n",
        "    elif mode_z == 'normal':\n",
        "        input_z = torch.randn(batch_size, z_size)\n",
        "    return input_z"
      ],
      "metadata": {
        "id": "ZoeDBiGVcSsw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "batch_size = 32\n",
        "dataloader = DataLoader(mnist_dataset, batch_size, shuffle=False)\n",
        "input_real, label = next(iter(dataloader))\n",
        "input_real = input_real.view(batch_size, -1)\n",
        "\n",
        "torch.manual_seed(1)\n",
        "mode_z = 'uniform'  # 'uniform' vs. 'normal'\n",
        "input_z = create_noise(batch_size, z_size, mode_z)\n",
        "\n",
        "print('input-z -- shape:', input_z.shape)\n",
        "print('input-real -- shape:', input_real.shape)\n",
        "\n",
        "g_output = gen_model(input_z)\n",
        "print('Output of G -- shape:', g_output.shape)\n",
        "\n",
        "d_proba_real = disc_model(input_real)\n",
        "d_proba_fake = disc_model(g_output)\n",
        "print('Disc. (real) -- shape:', d_proba_real.shape)\n",
        "print('Disc. (fake) -- shape:', d_proba_fake.shape)"
      ],
      "metadata": {
        "id": "fIoIIWXlcVUD",
        "outputId": "5166cc13-8009-4ed1-9f7b-3a7c79473403",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input-z -- shape: torch.Size([32, 20])\n",
            "input-real -- shape: torch.Size([32, 784])\n",
            "Output of G -- shape: torch.Size([32, 784])\n",
            "Disc. (real) -- shape: torch.Size([32, 1])\n",
            "Disc. (fake) -- shape: torch.Size([32, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "## Loss for the Generator\n",
        "g_labels_real = torch.ones_like(d_proba_fake)\n",
        "g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
        "print(f'Generator Loss: {g_loss:.4f}')\n",
        "\n",
        "## Loss for the Discriminator\n",
        "d_labels_real = torch.ones_like(d_proba_real)\n",
        "d_labels_fake = torch.zeros_like(d_proba_fake)\n",
        "\n",
        "d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
        "d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
        "print(f'Discriminator Losses: Real {d_loss_real:.4f} Fake {d_loss_fake:.4f}')"
      ],
      "metadata": {
        "id": "FpTL7ZnKcYcs",
        "outputId": "64216924-208c-47a7-bb0c-b0b15dc9e497",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generator Loss: 0.6983\n",
            "Discriminator Losses: Real 0.7479 Fake 0.6885\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "## Set up the dataset\n",
        "mnist_dl = DataLoader(mnist_dataset, batch_size=batch_size,\n",
        "                      shuffle=True, drop_last=True)\n",
        "\n",
        "## Set up the models\n",
        "gen_model = make_generator_network(\n",
        "    input_size=z_size,\n",
        "    num_hidden_layers=gen_hidden_layers,\n",
        "    num_hidden_units=gen_hidden_size,\n",
        "    num_output_units=np.prod(image_size)).to(device)\n",
        "\n",
        "disc_model = make_discriminator_network(\n",
        "    input_size=np.prod(image_size),\n",
        "    num_hidden_layers=disc_hidden_layers,\n",
        "    num_hidden_units=disc_hidden_size).to(device)\n",
        "\n",
        "## Loss function and optimizers:\n",
        "loss_fn = nn.BCELoss()\n",
        "g_optimizer = torch.optim.Adam(gen_model.parameters())\n",
        "d_optimizer = torch.optim.Adam(disc_model.parameters())"
      ],
      "metadata": {
        "id": "peuP7ptjcqsh"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Train the discriminator\n",
        "def d_train(x):\n",
        "    disc_model.zero_grad()\n",
        "\n",
        "    # Train discriminator with a real batch\n",
        "    batch_size = x.size(0)\n",
        "    x = x.view(batch_size, -1).to(device)\n",
        "    d_labels_real = torch.ones(batch_size, 1, device=device)\n",
        "\n",
        "    d_proba_real = disc_model(x)\n",
        "    d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
        "\n",
        "    # Train discriminator on a fake batch\n",
        "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "    g_output = gen_model(input_z)\n",
        "\n",
        "    d_proba_fake = disc_model(g_output)\n",
        "    d_labels_fake = torch.zeros(batch_size, 1, device=device)\n",
        "    d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
        "\n",
        "    # gradient backprop & optimize ONLY D's parameters\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "\n",
        "    return d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()"
      ],
      "metadata": {
        "id": "JI3HlWCjctG1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "## Train the generator\n",
        "def g_train(x):\n",
        "    gen_model.zero_grad()\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "    g_labels_real = torch.ones(batch_size, 1, device=device)\n",
        "\n",
        "    g_output = gen_model(input_z)\n",
        "    d_proba_fake = disc_model(g_output)\n",
        "    g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
        "\n",
        "    # gradient backprop & optimize ONLY G's parameters\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "\n",
        "    return g_loss.data.item()"
      ],
      "metadata": {
        "id": "Ocjx-QzicvMl"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "\n",
        "def create_samples(g_model, input_z):\n",
        "    g_output = g_model(input_z)\n",
        "    images = torch.reshape(g_output, (batch_size, *image_size))\n",
        "    return (images+1)/2.0\n",
        "\n",
        "epoch_samples = []\n",
        "\n",
        "all_d_losses = []\n",
        "all_g_losses = []\n",
        "\n",
        "all_d_real = []\n",
        "all_d_fake = []\n",
        "\n",
        "num_epochs = 100\n",
        "torch.manual_seed(1)\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    d_losses, g_losses = [], []\n",
        "    d_vals_real, d_vals_fake = [], []\n",
        "    for i, (x, _) in enumerate(mnist_dl):\n",
        "        d_loss, d_proba_real, d_proba_fake = d_train(x)\n",
        "        d_losses.append(d_loss)\n",
        "        g_losses.append(g_train(x))\n",
        "\n",
        "        d_vals_real.append(d_proba_real.mean().cpu())\n",
        "        d_vals_fake.append(d_proba_fake.mean().cpu())\n",
        "\n",
        "    all_d_losses.append(torch.tensor(d_losses).mean())\n",
        "    all_g_losses.append(torch.tensor(g_losses).mean())\n",
        "    all_d_real.append(torch.tensor(d_vals_real).mean())\n",
        "    all_d_fake.append(torch.tensor(d_vals_fake).mean())\n",
        "    print(f'Epoch {epoch:03d} | Avg Losses >>'\n",
        "          f' G/D {all_g_losses[-1]:.4f}/{all_d_losses[-1]:.4f}'\n",
        "          f' [D-Real: {all_d_real[-1]:.4f} D-Fake: {all_d_fake[-1]:.4f}]')\n",
        "    epoch_samples.append(\n",
        "        create_samples(gen_model, fixed_z).detach().cpu().numpy())"
      ],
      "metadata": {
        "id": "8b6o4GQNcyp8",
        "outputId": "2ac05623-2897-414e-8835-9ead275c88be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Avg Losses >> G/D 0.8944/0.9068 [D-Real: 0.8035 D-Fake: 0.4717]\n",
            "Epoch 002 | Avg Losses >> G/D 0.9469/1.1271 [D-Real: 0.6164 D-Fake: 0.4318]\n",
            "Epoch 003 | Avg Losses >> G/D 0.9596/1.1998 [D-Real: 0.5790 D-Fake: 0.4277]\n",
            "Epoch 004 | Avg Losses >> G/D 0.9415/1.2163 [D-Real: 0.5737 D-Fake: 0.4305]\n",
            "Epoch 005 | Avg Losses >> G/D 0.9270/1.2284 [D-Real: 0.5705 D-Fake: 0.4286]\n",
            "Epoch 006 | Avg Losses >> G/D 0.9453/1.2473 [D-Real: 0.5620 D-Fake: 0.4335]\n",
            "Epoch 007 | Avg Losses >> G/D 1.0020/1.1734 [D-Real: 0.5897 D-Fake: 0.4058]\n",
            "Epoch 008 | Avg Losses >> G/D 1.0015/1.1883 [D-Real: 0.5890 D-Fake: 0.4110]\n",
            "Epoch 009 | Avg Losses >> G/D 0.9548/1.2096 [D-Real: 0.5805 D-Fake: 0.4229]\n",
            "Epoch 010 | Avg Losses >> G/D 0.9074/1.2498 [D-Real: 0.5619 D-Fake: 0.4359]\n",
            "Epoch 011 | Avg Losses >> G/D 0.9841/1.2001 [D-Real: 0.5831 D-Fake: 0.4135]\n",
            "Epoch 012 | Avg Losses >> G/D 0.9437/1.2165 [D-Real: 0.5803 D-Fake: 0.4267]\n",
            "Epoch 013 | Avg Losses >> G/D 0.9947/1.1981 [D-Real: 0.5860 D-Fake: 0.4148]\n",
            "Epoch 014 | Avg Losses >> G/D 0.9812/1.2052 [D-Real: 0.5852 D-Fake: 0.4206]\n",
            "Epoch 015 | Avg Losses >> G/D 0.9600/1.2153 [D-Real: 0.5790 D-Fake: 0.4233]\n",
            "Epoch 016 | Avg Losses >> G/D 0.8970/1.2414 [D-Real: 0.5677 D-Fake: 0.4347]\n",
            "Epoch 017 | Avg Losses >> G/D 0.8782/1.2710 [D-Real: 0.5542 D-Fake: 0.4448]\n",
            "Epoch 018 | Avg Losses >> G/D 0.8481/1.2878 [D-Real: 0.5478 D-Fake: 0.4529]\n",
            "Epoch 019 | Avg Losses >> G/D 0.8366/1.2874 [D-Real: 0.5474 D-Fake: 0.4540]\n",
            "Epoch 020 | Avg Losses >> G/D 0.8333/1.2935 [D-Real: 0.5441 D-Fake: 0.4560]\n",
            "Epoch 021 | Avg Losses >> G/D 0.8181/1.3075 [D-Real: 0.5369 D-Fake: 0.4588]\n",
            "Epoch 022 | Avg Losses >> G/D 0.7901/1.3261 [D-Real: 0.5300 D-Fake: 0.4687]\n",
            "Epoch 023 | Avg Losses >> G/D 0.8039/1.3242 [D-Real: 0.5307 D-Fake: 0.4668]\n",
            "Epoch 024 | Avg Losses >> G/D 0.7880/1.3316 [D-Real: 0.5269 D-Fake: 0.4704]\n",
            "Epoch 025 | Avg Losses >> G/D 0.7935/1.3272 [D-Real: 0.5289 D-Fake: 0.4691]\n",
            "Epoch 026 | Avg Losses >> G/D 0.7931/1.3283 [D-Real: 0.5292 D-Fake: 0.4701]\n",
            "Epoch 027 | Avg Losses >> G/D 0.8068/1.3142 [D-Real: 0.5351 D-Fake: 0.4645]\n",
            "Epoch 028 | Avg Losses >> G/D 0.7785/1.3344 [D-Real: 0.5266 D-Fake: 0.4737]\n",
            "Epoch 029 | Avg Losses >> G/D 0.8083/1.3146 [D-Real: 0.5350 D-Fake: 0.4651]\n",
            "Epoch 030 | Avg Losses >> G/D 0.8272/1.2943 [D-Real: 0.5458 D-Fake: 0.4592]\n",
            "Epoch 031 | Avg Losses >> G/D 0.8349/1.2993 [D-Real: 0.5433 D-Fake: 0.4593]\n",
            "Epoch 032 | Avg Losses >> G/D 0.8097/1.3111 [D-Real: 0.5375 D-Fake: 0.4647]\n",
            "Epoch 033 | Avg Losses >> G/D 0.7796/1.3282 [D-Real: 0.5285 D-Fake: 0.4711]\n",
            "Epoch 034 | Avg Losses >> G/D 0.7908/1.3272 [D-Real: 0.5298 D-Fake: 0.4705]\n",
            "Epoch 035 | Avg Losses >> G/D 0.7892/1.3293 [D-Real: 0.5282 D-Fake: 0.4692]\n",
            "Epoch 036 | Avg Losses >> G/D 0.7683/1.3390 [D-Real: 0.5240 D-Fake: 0.4765]\n",
            "Epoch 037 | Avg Losses >> G/D 0.7939/1.3240 [D-Real: 0.5314 D-Fake: 0.4692]\n",
            "Epoch 038 | Avg Losses >> G/D 0.7977/1.3156 [D-Real: 0.5347 D-Fake: 0.4666]\n",
            "Epoch 039 | Avg Losses >> G/D 0.7768/1.3358 [D-Real: 0.5260 D-Fake: 0.4740]\n",
            "Epoch 040 | Avg Losses >> G/D 0.7668/1.3419 [D-Real: 0.5221 D-Fake: 0.4767]\n",
            "Epoch 041 | Avg Losses >> G/D 0.7782/1.3348 [D-Real: 0.5270 D-Fake: 0.4733]\n",
            "Epoch 042 | Avg Losses >> G/D 0.7911/1.3277 [D-Real: 0.5295 D-Fake: 0.4699]\n",
            "Epoch 043 | Avg Losses >> G/D 0.8046/1.3195 [D-Real: 0.5340 D-Fake: 0.4669]\n",
            "Epoch 044 | Avg Losses >> G/D 0.7896/1.3253 [D-Real: 0.5300 D-Fake: 0.4702]\n",
            "Epoch 045 | Avg Losses >> G/D 0.8001/1.3171 [D-Real: 0.5338 D-Fake: 0.4664]\n",
            "Epoch 046 | Avg Losses >> G/D 0.8264/1.3015 [D-Real: 0.5424 D-Fake: 0.4603]\n",
            "Epoch 047 | Avg Losses >> G/D 0.8166/1.3105 [D-Real: 0.5370 D-Fake: 0.4632]\n",
            "Epoch 048 | Avg Losses >> G/D 0.8237/1.3064 [D-Real: 0.5387 D-Fake: 0.4608]\n",
            "Epoch 049 | Avg Losses >> G/D 0.7723/1.3331 [D-Real: 0.5264 D-Fake: 0.4738]\n",
            "Epoch 050 | Avg Losses >> G/D 0.7732/1.3403 [D-Real: 0.5234 D-Fake: 0.4752]\n",
            "Epoch 051 | Avg Losses >> G/D 0.7602/1.3464 [D-Real: 0.5195 D-Fake: 0.4777]\n",
            "Epoch 052 | Avg Losses >> G/D 0.7693/1.3406 [D-Real: 0.5234 D-Fake: 0.4766]\n",
            "Epoch 053 | Avg Losses >> G/D 0.7828/1.3309 [D-Real: 0.5286 D-Fake: 0.4731]\n",
            "Epoch 054 | Avg Losses >> G/D 0.7809/1.3334 [D-Real: 0.5276 D-Fake: 0.4738]\n",
            "Epoch 055 | Avg Losses >> G/D 0.7710/1.3389 [D-Real: 0.5251 D-Fake: 0.4760]\n",
            "Epoch 056 | Avg Losses >> G/D 0.7655/1.3407 [D-Real: 0.5229 D-Fake: 0.4769]\n",
            "Epoch 057 | Avg Losses >> G/D 0.7743/1.3387 [D-Real: 0.5244 D-Fake: 0.4741]\n",
            "Epoch 058 | Avg Losses >> G/D 0.7699/1.3378 [D-Real: 0.5252 D-Fake: 0.4758]\n",
            "Epoch 059 | Avg Losses >> G/D 0.7731/1.3367 [D-Real: 0.5258 D-Fake: 0.4755]\n",
            "Epoch 060 | Avg Losses >> G/D 0.7832/1.3297 [D-Real: 0.5286 D-Fake: 0.4724]\n",
            "Epoch 061 | Avg Losses >> G/D 0.7933/1.3270 [D-Real: 0.5300 D-Fake: 0.4703]\n",
            "Epoch 062 | Avg Losses >> G/D 0.7815/1.3288 [D-Real: 0.5291 D-Fake: 0.4720]\n",
            "Epoch 063 | Avg Losses >> G/D 0.7950/1.3249 [D-Real: 0.5309 D-Fake: 0.4700]\n",
            "Epoch 064 | Avg Losses >> G/D 0.8003/1.3193 [D-Real: 0.5331 D-Fake: 0.4669]\n",
            "Epoch 065 | Avg Losses >> G/D 0.7817/1.3272 [D-Real: 0.5298 D-Fake: 0.4722]\n",
            "Epoch 066 | Avg Losses >> G/D 0.7910/1.3277 [D-Real: 0.5293 D-Fake: 0.4702]\n",
            "Epoch 067 | Avg Losses >> G/D 0.8080/1.3159 [D-Real: 0.5344 D-Fake: 0.4653]\n",
            "Epoch 068 | Avg Losses >> G/D 0.7940/1.3221 [D-Real: 0.5317 D-Fake: 0.4670]\n",
            "Epoch 069 | Avg Losses >> G/D 0.7888/1.3298 [D-Real: 0.5285 D-Fake: 0.4703]\n",
            "Epoch 070 | Avg Losses >> G/D 0.8063/1.3150 [D-Real: 0.5356 D-Fake: 0.4659]\n",
            "Epoch 071 | Avg Losses >> G/D 0.7977/1.3221 [D-Real: 0.5320 D-Fake: 0.4674]\n",
            "Epoch 072 | Avg Losses >> G/D 0.7812/1.3301 [D-Real: 0.5285 D-Fake: 0.4721]\n",
            "Epoch 073 | Avg Losses >> G/D 0.7782/1.3343 [D-Real: 0.5263 D-Fake: 0.4740]\n",
            "Epoch 074 | Avg Losses >> G/D 0.8103/1.3208 [D-Real: 0.5331 D-Fake: 0.4660]\n",
            "Epoch 075 | Avg Losses >> G/D 0.7995/1.3207 [D-Real: 0.5323 D-Fake: 0.4674]\n",
            "Epoch 076 | Avg Losses >> G/D 0.8171/1.3076 [D-Real: 0.5392 D-Fake: 0.4627]\n",
            "Epoch 077 | Avg Losses >> G/D 0.8173/1.3073 [D-Real: 0.5391 D-Fake: 0.4634]\n",
            "Epoch 078 | Avg Losses >> G/D 0.8071/1.3136 [D-Real: 0.5369 D-Fake: 0.4649]\n",
            "Epoch 079 | Avg Losses >> G/D 0.8138/1.3125 [D-Real: 0.5375 D-Fake: 0.4644]\n",
            "Epoch 080 | Avg Losses >> G/D 0.8068/1.3150 [D-Real: 0.5345 D-Fake: 0.4651]\n",
            "Epoch 081 | Avg Losses >> G/D 0.7885/1.3260 [D-Real: 0.5302 D-Fake: 0.4706]\n",
            "Epoch 082 | Avg Losses >> G/D 0.8038/1.3203 [D-Real: 0.5336 D-Fake: 0.4671]\n",
            "Epoch 083 | Avg Losses >> G/D 0.8016/1.3235 [D-Real: 0.5315 D-Fake: 0.4674]\n",
            "Epoch 084 | Avg Losses >> G/D 0.7914/1.3291 [D-Real: 0.5286 D-Fake: 0.4700]\n",
            "Epoch 085 | Avg Losses >> G/D 0.7910/1.3261 [D-Real: 0.5299 D-Fake: 0.4696]\n",
            "Epoch 086 | Avg Losses >> G/D 0.8081/1.3168 [D-Real: 0.5347 D-Fake: 0.4656]\n"
          ]
        }
      ]
    }
  ]
}