{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdbATDxVtWsJ6+P9W8CfP7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeohshin/ML_with_Pytorch_Sklearn_rasbt/blob/main/ch17_transposeconvolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DCGAN- Deep Convolution Generative Adversarial Netowkr"
      ],
      "metadata": {
        "id": "lBwGOHbuYcvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCyK-8lMYoYl",
        "outputId": "aab35336-f30b-42e1-b260-08d66e93e57e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.1+cu121\n",
            "GPU Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "SMgPEJkyY-4t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Mfa5_okZg_8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train the DCGAN model"
      ],
      "metadata": {
        "id": "JhvHRDgAZigc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "image_path = './'\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5))\n",
        "])\n",
        "\n",
        "mnist_dataset = torchvision.datasets.MNIST(root=image_path,\n",
        "                                           train=True,\n",
        "                                           transform=transform,\n",
        "                                           download=False)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "## Set up the dataset\n",
        "from torch.utils.data import DataLoader\n",
        "mnist_dl = DataLoader(mnist_dataset, batch_size=batch_size,\n",
        "                      shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "YhK3kmu1ZkdW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions"
      ],
      "metadata": {
        "id": "iraDLxVvc-RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_network(input_size, n_filters):\n",
        "    model = nn.Sequential(\n",
        "        nn.ConvTranspose2d(input_size, n_filters*4, 4, 1, 0,\n",
        "                           bias=False),\n",
        "        nn.BatchNorm2d(n_filters*4),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(n_filters*4, n_filters*2, 3, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_filters*2),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(n_filters*2, n_filters, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_filters),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(n_filters, 1, 4, 2, 1, bias=False),\n",
        "        nn.Tanh())\n",
        "    return model\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_filters):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, n_filters, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(n_filters, n_filters*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(n_filters *2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(n_filters*2, n_filters*4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(n_filters*4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(n_filters*4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.network(input)\n",
        "        return output.view(-1, 1).squeeze(0)"
      ],
      "metadata": {
        "id": "axgeuTWQby5E"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_size = 100\n",
        "image_size = (28, 28)\n",
        "n_filters = 32\n",
        "gen_model = make_generator_network(z_size, n_filters).to(device)\n",
        "print(gen_model)\n",
        "disc_model = Discriminator(n_filters).to(device)\n",
        "print(disc_model)"
      ],
      "metadata": {
        "id": "9d5ImyP6fcJH",
        "outputId": "def6fe40-4d32-4931-c6c1-3957796f1c7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): ConvTranspose2d(100, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): LeakyReLU(negative_slope=0.2)\n",
            "  (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): LeakyReLU(negative_slope=0.2)\n",
            "  (6): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (8): LeakyReLU(negative_slope=0.2)\n",
            "  (9): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (10): Tanh()\n",
            ")\n",
            "Discriminator(\n",
            "  (network): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (9): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss function and optimizers:\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "g_optimizer = torch.optim.Adam(gen_model.parameters(), 0.0003)\n",
        "d_optimizer = torch.optim.Adam(disc_model.parameters(), 0.0002)\n"
      ],
      "metadata": {
        "id": "ej4PdiWtfCpU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_noise(batch_size, z_size, mode_z):\n",
        "    if mode_z == 'uniform':\n",
        "        input_z = torch.rand(batch_size, z_size, 1, 1)*2 -1\n",
        "    elif mode_z == 'normal':\n",
        "        input_z = torch.randn(batch_size, z_size, 1, 1)\n",
        "    return input_z"
      ],
      "metadata": {
        "id": "HFMGMXApkLmX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d_train(x):\n",
        "    disc_model.zero_grad()\n",
        "\n",
        "    #Train discriminator with real batch\n",
        "    batch_size = x.size(0)\n",
        "    x = x.to(device)\n",
        "    d_labels_real = torch.ones(batch_size, 1, device=device)\n",
        "\n",
        "    d_proba_real = disc_model(x)\n",
        "    d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
        "\n",
        "    #Train discriminator with fake batch\n",
        "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "    g_output = gen_model(input_z)\n",
        "\n",
        "    d_proba_fake = disc_model(g_output)\n",
        "    d_labels_fake = torch.zeros(batch_size, 1, device=device)\n",
        "    d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
        "\n",
        "    #gradient backpro & optimize ONLY D's parameters\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "\n",
        "    return d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()"
      ],
      "metadata": {
        "id": "AgnPs9IKkbTg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train the generator\n",
        "\n",
        "def g_train(x):\n",
        "    gen_model.zero_grad()\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "    g_labels_real = torch.ones((batch_size, 1), device=device)\n",
        "\n",
        "    g_output = gen_model(input_z)\n",
        "    d_proba_fake = disc_model(g_output)\n",
        "    g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
        "\n",
        "    #gradient backprop & optimize ONLY G's parameters\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "\n",
        "    return g_loss.data.item()"
      ],
      "metadata": {
        "id": "QggdnuAdlntw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mode_z = 'uniform'\n",
        "fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "\n",
        "def create_samples(g_model, input_z):\n",
        "    g_output = g_model(input_z)\n",
        "    images = torch.reshape(g_output, (batch_size, *image_size))\n",
        "    return (images+1)/2.0\n",
        "\n",
        "epoch_samples = []\n",
        "\n",
        "num_epochs = 100\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    gen_model.train()\n",
        "    d_losses, g_losses = [], []\n",
        "    for i, (x, _) in enumerate(mnist_dl):\n",
        "        d_loss, d_proba_real, d_proba_fake = d_train(x)\n",
        "        d_losses.append(d_loss)\n",
        "        g_losses.append(g_train(x))\n",
        "\n",
        "    print(f'Epoch {epoch:03d} | Avg Losses >>'\n",
        "          f' G/D {torch.FloatTensor(g_losses).mean():.4f}'\n",
        "          f'/{torch.FloatTensor(d_losses).mean():.4f}')\n",
        "    gen_model.eval()\n",
        "    epoch_samples.append(\n",
        "        create_samples(gen_model, fixed_z).detach().cpu().numpy())"
      ],
      "metadata": {
        "id": "fPOz5cjimiPy",
        "outputId": "8a4e3fd9-7c0e-4b1c-bc5d-cea2968e1dac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Avg Losses >> G/D 4.4786/0.1366\n",
            "Epoch 002 | Avg Losses >> G/D 4.0268/0.1901\n",
            "Epoch 003 | Avg Losses >> G/D 3.4841/0.3210\n",
            "Epoch 004 | Avg Losses >> G/D 3.0257/0.3580\n",
            "Epoch 005 | Avg Losses >> G/D 2.8294/0.3411\n",
            "Epoch 006 | Avg Losses >> G/D 2.8886/0.3486\n",
            "Epoch 007 | Avg Losses >> G/D 2.9302/0.3166\n",
            "Epoch 008 | Avg Losses >> G/D 2.9157/0.3129\n",
            "Epoch 009 | Avg Losses >> G/D 3.0121/0.3117\n",
            "Epoch 010 | Avg Losses >> G/D 3.0065/0.3113\n",
            "Epoch 011 | Avg Losses >> G/D 3.0379/0.2774\n",
            "Epoch 012 | Avg Losses >> G/D 3.0528/0.2867\n",
            "Epoch 013 | Avg Losses >> G/D 3.1833/0.2465\n",
            "Epoch 014 | Avg Losses >> G/D 3.2388/0.2707\n",
            "Epoch 015 | Avg Losses >> G/D 3.3302/0.2237\n",
            "Epoch 016 | Avg Losses >> G/D 3.3297/0.2503\n",
            "Epoch 017 | Avg Losses >> G/D 3.4730/0.2108\n",
            "Epoch 018 | Avg Losses >> G/D 3.4842/0.2112\n",
            "Epoch 019 | Avg Losses >> G/D 3.5055/0.2111\n",
            "Epoch 020 | Avg Losses >> G/D 3.6385/0.2019\n",
            "Epoch 021 | Avg Losses >> G/D 3.6104/0.2360\n",
            "Epoch 022 | Avg Losses >> G/D 3.6644/0.1969\n",
            "Epoch 023 | Avg Losses >> G/D 3.7123/0.1954\n",
            "Epoch 024 | Avg Losses >> G/D 3.7266/0.2031\n",
            "Epoch 025 | Avg Losses >> G/D 3.7938/0.1822\n",
            "Epoch 026 | Avg Losses >> G/D 3.8341/0.1657\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "grdlBWCpmlLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}