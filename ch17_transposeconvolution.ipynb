{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNdbATDxVtWsJ6+P9W8CfP7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeohshin/ML_with_Pytorch_Sklearn_rasbt/blob/main/ch17_transposeconvolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DCGAN- Deep Convolution Generative Adversarial Netowkr"
      ],
      "metadata": {
        "id": "lBwGOHbuYcvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCyK-8lMYoYl",
        "outputId": "aab35336-f30b-42e1-b260-08d66e93e57e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.1+cu121\n",
            "GPU Available: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "SMgPEJkyY-4t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Mfa5_okZg_8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train the DCGAN model"
      ],
      "metadata": {
        "id": "JhvHRDgAZigc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "image_path = './'\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5))\n",
        "])\n",
        "\n",
        "mnist_dataset = torchvision.datasets.MNIST(root=image_path,\n",
        "                                           train=True,\n",
        "                                           transform=transform,\n",
        "                                           download=False)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "## Set up the dataset\n",
        "from torch.utils.data import DataLoader\n",
        "mnist_dl = DataLoader(mnist_dataset, batch_size=batch_size,\n",
        "                      shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "id": "YhK3kmu1ZkdW"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions"
      ],
      "metadata": {
        "id": "iraDLxVvc-RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_network(input_size, n_filters):\n",
        "    model = nn.Sequential(\n",
        "        nn.ConvTranspose2d(input_size, n_filters*4, 4, 1, 0,\n",
        "                           bias=False),\n",
        "        nn.BatchNorm2d(n_filters*4),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(n_filters*4, n_filters*2, 3, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_filters*2),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(n_filters*2, n_filters, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_filters),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(n_filters, 1, 4, 2, 1, bias=False),\n",
        "        nn.Tanh())\n",
        "    return model\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_filters):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, n_filters, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(n_filters, n_filters*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(n_filters *2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(n_filters*2, n_filters*4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(n_filters*4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(n_filters*4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.network(input)\n",
        "        return output.view(-1, 1).squeeze(0)"
      ],
      "metadata": {
        "id": "axgeuTWQby5E"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_size = 100\n",
        "image_size = (28, 28)\n",
        "n_filters = 32\n",
        "gen_model = make_generator_network(z_size, n_filters).to(device)\n",
        "print(gen_model)\n",
        "disc_model = Discriminator(n_filters).to(device)\n",
        "print(disc_model)"
      ],
      "metadata": {
        "id": "9d5ImyP6fcJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "def6fe40-4d32-4931-c6c1-3957796f1c7d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): ConvTranspose2d(100, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): LeakyReLU(negative_slope=0.2)\n",
            "  (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): LeakyReLU(negative_slope=0.2)\n",
            "  (6): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (8): LeakyReLU(negative_slope=0.2)\n",
            "  (9): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (10): Tanh()\n",
            ")\n",
            "Discriminator(\n",
            "  (network): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (9): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss function and optimizers:\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "g_optimizer = torch.optim.Adam(gen_model.parameters(), 0.0003)\n",
        "d_optimizer = torch.optim.Adam(disc_model.parameters(), 0.0002)\n"
      ],
      "metadata": {
        "id": "ej4PdiWtfCpU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_noise(batch_size, z_size, mode_z):\n",
        "    if mode_z == 'uniform':\n",
        "        input_z = torch.rand(batch_size, z_size, 1, 1)*2 -1\n",
        "    elif mode_z == 'normal':\n",
        "        input_z = torch.randn(batch_size, z_size, 1, 1)\n",
        "    return input_z"
      ],
      "metadata": {
        "id": "HFMGMXApkLmX"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d_train(x):\n",
        "    disc_model.zero_grad()\n",
        "\n",
        "    #Train discriminator with real batch\n",
        "    batch_size = x.size(0)\n",
        "    x = x.to(device)\n",
        "    d_labels_real = torch.ones(batch_size, 1, device=device)\n",
        "\n",
        "    d_proba_real = disc_model(x)\n",
        "    d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
        "\n",
        "    #Train discriminator with fake batch\n",
        "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "    g_output = gen_model(input_z)\n",
        "\n",
        "    d_proba_fake = disc_model(g_output)\n",
        "    d_labels_fake = torch.zeros(batch_size, 1, device=device)\n",
        "    d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
        "\n",
        "    #gradient backpro & optimize ONLY D's parameters\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "\n",
        "    return d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()"
      ],
      "metadata": {
        "id": "AgnPs9IKkbTg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train the generator\n",
        "\n",
        "def g_train(x):\n",
        "    gen_model.zero_grad()\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "    g_labels_real = torch.ones((batch_size, 1), device=device)\n",
        "\n",
        "    g_output = gen_model(input_z)\n",
        "    d_proba_fake = disc_model(g_output)\n",
        "    g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
        "\n",
        "    #gradient backprop & optimize ONLY G's parameters\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "\n",
        "    return g_loss.data.item()"
      ],
      "metadata": {
        "id": "QggdnuAdlntw"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mode_z = 'uniform'\n",
        "fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "\n",
        "def create_samples(g_model, input_z):\n",
        "    g_output = g_model(input_z)\n",
        "    images = torch.reshape(g_output, (batch_size, *image_size))\n",
        "    return (images+1)/2.0\n",
        "\n",
        "epoch_samples = []\n",
        "\n",
        "num_epochs = 100\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    gen_model.train()\n",
        "    d_losses, g_losses = [], []\n",
        "    for i, (x, _) in enumerate(mnist_dl):\n",
        "        d_loss, d_proba_real, d_proba_fake = d_train(x)\n",
        "        d_losses.append(d_loss)\n",
        "        g_losses.append(g_train(x))\n",
        "\n",
        "    print(f'Epoch {epoch:03d} | Avg Losses >>'\n",
        "          f' G/D {torch.FloatTensor(g_losses).mean():.4f}'\n",
        "          f'/{torch.FloatTensor(d_losses).mean():.4f}')\n",
        "    gen_model.eval()\n",
        "    epoch_samples.append(\n",
        "        create_samples(gen_model, fixed_z).detach().cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPOz5cjimiPy",
        "outputId": "8a4e3fd9-7c0e-4b1c-bc5d-cea2968e1dac"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Avg Losses >> G/D 4.4786/0.1366\n",
            "Epoch 002 | Avg Losses >> G/D 4.0268/0.1901\n",
            "Epoch 003 | Avg Losses >> G/D 3.4841/0.3210\n",
            "Epoch 004 | Avg Losses >> G/D 3.0257/0.3580\n",
            "Epoch 005 | Avg Losses >> G/D 2.8294/0.3411\n",
            "Epoch 006 | Avg Losses >> G/D 2.8886/0.3486\n",
            "Epoch 007 | Avg Losses >> G/D 2.9302/0.3166\n",
            "Epoch 008 | Avg Losses >> G/D 2.9157/0.3129\n",
            "Epoch 009 | Avg Losses >> G/D 3.0121/0.3117\n",
            "Epoch 010 | Avg Losses >> G/D 3.0065/0.3113\n",
            "Epoch 011 | Avg Losses >> G/D 3.0379/0.2774\n",
            "Epoch 012 | Avg Losses >> G/D 3.0528/0.2867\n",
            "Epoch 013 | Avg Losses >> G/D 3.1833/0.2465\n",
            "Epoch 014 | Avg Losses >> G/D 3.2388/0.2707\n",
            "Epoch 015 | Avg Losses >> G/D 3.3302/0.2237\n",
            "Epoch 016 | Avg Losses >> G/D 3.3297/0.2503\n",
            "Epoch 017 | Avg Losses >> G/D 3.4730/0.2108\n",
            "Epoch 018 | Avg Losses >> G/D 3.4842/0.2112\n",
            "Epoch 019 | Avg Losses >> G/D 3.5055/0.2111\n",
            "Epoch 020 | Avg Losses >> G/D 3.6385/0.2019\n",
            "Epoch 021 | Avg Losses >> G/D 3.6104/0.2360\n",
            "Epoch 022 | Avg Losses >> G/D 3.6644/0.1969\n",
            "Epoch 023 | Avg Losses >> G/D 3.7123/0.1954\n",
            "Epoch 024 | Avg Losses >> G/D 3.7266/0.2031\n",
            "Epoch 025 | Avg Losses >> G/D 3.7938/0.1822\n",
            "Epoch 026 | Avg Losses >> G/D 3.8341/0.1657\n",
            "Epoch 027 | Avg Losses >> G/D 3.8214/0.1894\n",
            "Epoch 028 | Avg Losses >> G/D 3.9244/0.1722\n",
            "Epoch 029 | Avg Losses >> G/D 3.9016/0.2031\n",
            "Epoch 030 | Avg Losses >> G/D 4.0017/0.1721\n",
            "Epoch 031 | Avg Losses >> G/D 4.0552/0.1872\n",
            "Epoch 032 | Avg Losses >> G/D 3.9590/0.1796\n",
            "Epoch 033 | Avg Losses >> G/D 4.0993/0.1682\n",
            "Epoch 034 | Avg Losses >> G/D 4.1920/0.1483\n",
            "Epoch 035 | Avg Losses >> G/D 4.1610/0.1694\n",
            "Epoch 036 | Avg Losses >> G/D 4.1377/0.1590\n",
            "Epoch 037 | Avg Losses >> G/D 4.1795/0.1830\n",
            "Epoch 038 | Avg Losses >> G/D 4.1729/0.1389\n",
            "Epoch 039 | Avg Losses >> G/D 4.2159/0.1656\n",
            "Epoch 040 | Avg Losses >> G/D 4.2661/0.1585\n",
            "Epoch 041 | Avg Losses >> G/D 4.2838/0.1581\n",
            "Epoch 042 | Avg Losses >> G/D 4.4527/0.1194\n",
            "Epoch 043 | Avg Losses >> G/D 4.3774/0.1417\n",
            "Epoch 044 | Avg Losses >> G/D 4.4164/0.1553\n",
            "Epoch 045 | Avg Losses >> G/D 4.3753/0.1428\n",
            "Epoch 046 | Avg Losses >> G/D 4.5100/0.1561\n",
            "Epoch 047 | Avg Losses >> G/D 4.4409/0.1165\n",
            "Epoch 048 | Avg Losses >> G/D 4.4780/0.1502\n",
            "Epoch 049 | Avg Losses >> G/D 4.4094/0.1321\n",
            "Epoch 050 | Avg Losses >> G/D 4.4224/0.1504\n",
            "Epoch 051 | Avg Losses >> G/D 4.4204/0.1778\n",
            "Epoch 052 | Avg Losses >> G/D 4.5404/0.1033\n",
            "Epoch 053 | Avg Losses >> G/D 4.4916/0.1585\n",
            "Epoch 054 | Avg Losses >> G/D 4.5081/0.1282\n",
            "Epoch 055 | Avg Losses >> G/D 4.6426/0.1170\n",
            "Epoch 056 | Avg Losses >> G/D 4.6237/0.1347\n",
            "Epoch 057 | Avg Losses >> G/D 4.5267/0.1471\n",
            "Epoch 058 | Avg Losses >> G/D 4.5602/0.1400\n",
            "Epoch 059 | Avg Losses >> G/D 4.6989/0.1249\n",
            "Epoch 060 | Avg Losses >> G/D 4.8149/0.1030\n",
            "Epoch 061 | Avg Losses >> G/D 4.7090/0.1399\n",
            "Epoch 062 | Avg Losses >> G/D 4.7437/0.1227\n",
            "Epoch 063 | Avg Losses >> G/D 4.7325/0.1332\n",
            "Epoch 064 | Avg Losses >> G/D 4.8108/0.1409\n",
            "Epoch 065 | Avg Losses >> G/D 4.6693/0.1197\n",
            "Epoch 066 | Avg Losses >> G/D 4.8802/0.1102\n",
            "Epoch 067 | Avg Losses >> G/D 4.9691/0.1038\n",
            "Epoch 068 | Avg Losses >> G/D 4.8750/0.1233\n",
            "Epoch 069 | Avg Losses >> G/D 4.7992/0.1184\n",
            "Epoch 070 | Avg Losses >> G/D 4.8772/0.1103\n",
            "Epoch 071 | Avg Losses >> G/D 4.8370/0.1366\n",
            "Epoch 072 | Avg Losses >> G/D 4.7880/0.1083\n",
            "Epoch 073 | Avg Losses >> G/D 4.9236/0.1102\n",
            "Epoch 074 | Avg Losses >> G/D 5.0543/0.0810\n",
            "Epoch 075 | Avg Losses >> G/D 4.8722/0.1338\n",
            "Epoch 076 | Avg Losses >> G/D 4.9895/0.1097\n",
            "Epoch 077 | Avg Losses >> G/D 5.0354/0.1217\n",
            "Epoch 078 | Avg Losses >> G/D 4.9067/0.1039\n",
            "Epoch 079 | Avg Losses >> G/D 5.0250/0.1270\n",
            "Epoch 080 | Avg Losses >> G/D 5.0324/0.1287\n",
            "Epoch 081 | Avg Losses >> G/D 4.8981/0.1130\n",
            "Epoch 082 | Avg Losses >> G/D 5.0094/0.0970\n",
            "Epoch 083 | Avg Losses >> G/D 5.0557/0.1297\n",
            "Epoch 084 | Avg Losses >> G/D 4.9402/0.0990\n",
            "Epoch 085 | Avg Losses >> G/D 4.9401/0.1447\n",
            "Epoch 086 | Avg Losses >> G/D 4.8281/0.1267\n",
            "Epoch 087 | Avg Losses >> G/D 5.0780/0.0848\n",
            "Epoch 088 | Avg Losses >> G/D 5.1951/0.0896\n",
            "Epoch 089 | Avg Losses >> G/D 5.0507/0.1301\n",
            "Epoch 090 | Avg Losses >> G/D 5.1208/0.0868\n",
            "Epoch 091 | Avg Losses >> G/D 5.0784/0.1084\n",
            "Epoch 092 | Avg Losses >> G/D 5.1515/0.1287\n",
            "Epoch 093 | Avg Losses >> G/D 5.2415/0.0914\n",
            "Epoch 094 | Avg Losses >> G/D 5.3216/0.0774\n",
            "Epoch 095 | Avg Losses >> G/D 5.0438/0.1388\n",
            "Epoch 096 | Avg Losses >> G/D 5.0888/0.0960\n",
            "Epoch 097 | Avg Losses >> G/D 5.0336/0.1548\n",
            "Epoch 098 | Avg Losses >> G/D 5.4124/0.0624\n",
            "Epoch 099 | Avg Losses >> G/D 5.3180/0.0902\n",
            "Epoch 100 | Avg Losses >> G/D 5.2986/0.0936\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "grdlBWCpmlLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}