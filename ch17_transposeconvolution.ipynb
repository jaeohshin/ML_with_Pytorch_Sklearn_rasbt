{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMvdEoiVRYhixLiXRts8TXu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaeohshin/ML_with_Pytorch_Sklearn_rasbt/blob/main/ch17_transposeconvolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DCGAN- Deep Convolution Generative Adversarial Netowkr"
      ],
      "metadata": {
        "id": "lBwGOHbuYcvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(\"GPU Available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")\n",
        "else:\n",
        "    device = \"cpu\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCyK-8lMYoYl",
        "outputId": "7a96dab0-e8e1-4515-c700-6a6cf2e2ab76"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.1+cu121\n",
            "GPU Available: False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "SMgPEJkyY-4t"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Mfa5_okZg_8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train the DCGAN model"
      ],
      "metadata": {
        "id": "JhvHRDgAZigc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "image_path = './'\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.5), std=(0.5))\n",
        "])\n",
        "\n",
        "mnist_dataset = torchvision.datasets.MNIST(root=image_path,\n",
        "                                           train=True,\n",
        "                                           transform=transform,\n",
        "                                           download=True)\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "torch.manual_seed(1)\n",
        "np.random.seed(1)\n",
        "\n",
        "## Set up the dataset\n",
        "from torch.utils.data import DataLoader\n",
        "mnist_dl = DataLoader(mnist_dataset, batch_size=batch_size,\n",
        "                      shuffle=True, drop_last=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhK3kmu1ZkdW",
        "outputId": "083828b3-214d-4973-bfd9-36ec232bff3f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 10604993.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1848355.80it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 13690794.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2561587.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper functions"
      ],
      "metadata": {
        "id": "iraDLxVvc-RA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_generator_network(input_size, n_filters):\n",
        "    model = nn.Sequential(\n",
        "        nn.ConvTranspose2d(input_size, n_filters*4, 4, 1, 0,\n",
        "                           bias=False),\n",
        "        nn.BatchNorm2d(n_filters*4),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(n_filters*4, n_filters*2, 3, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_filters*2),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(n_filters*2, n_filters, 4, 2, 1, bias=False),\n",
        "        nn.BatchNorm2d(n_filters),\n",
        "        nn.LeakyReLU(0.2),\n",
        "\n",
        "        nn.ConvTranspose2d(n_filters, 1, 4, 2, 1, bias=False),\n",
        "        nn.Tanh())\n",
        "    return model\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, n_filters):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(1, n_filters, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(n_filters, n_filters*2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(n_filters *2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(n_filters*2, n_filters*4, 3, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(n_filters*4),\n",
        "            nn.LeakyReLU(0.2),\n",
        "\n",
        "            nn.Conv2d(n_filters*4, 1, 4, 1, 0, bias=False),\n",
        "            nn.Sigmoid())\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = self.network(input)\n",
        "        return output.view(-1, 1).squeeze(0)"
      ],
      "metadata": {
        "id": "axgeuTWQby5E"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z_size = 100\n",
        "image_size = (28, 28)\n",
        "n_filters = 32\n",
        "gen_model = make_generator_network(z_size, n_filters).to(device)\n",
        "print(gen_model)\n",
        "disc_model = Discriminator(n_filters).to(device)\n",
        "print(disc_model)"
      ],
      "metadata": {
        "id": "9d5ImyP6fcJH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8be18d6-3292-4a38-f505-96e3c80c6590"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequential(\n",
            "  (0): ConvTranspose2d(100, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (2): LeakyReLU(negative_slope=0.2)\n",
            "  (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (5): LeakyReLU(negative_slope=0.2)\n",
            "  (6): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (8): LeakyReLU(negative_slope=0.2)\n",
            "  (9): ConvTranspose2d(32, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "  (10): Tanh()\n",
            ")\n",
            "Discriminator(\n",
            "  (network): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (1): LeakyReLU(negative_slope=0.2)\n",
            "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (6): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2)\n",
            "    (8): Conv2d(128, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (9): Sigmoid()\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Loss function and optimizers:\n",
        "\n",
        "loss_fn = nn.BCELoss()\n",
        "g_optimizer = torch.optim.Adam(gen_model.parameters(), 0.0003)\n",
        "d_optimizer = torch.optim.Adam(disc_model.parameters(), 0.0002)\n"
      ],
      "metadata": {
        "id": "ej4PdiWtfCpU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_noise(batch_size, z_size, mode_z):\n",
        "    if mode_z == 'uniform':\n",
        "        input_z = torch.rand(batch_size, z_size, 1, 1)*2 -1\n",
        "    elif mode_z == 'normal':\n",
        "        input_z = torch.randn(batch_size, z_size, 1, 1)\n",
        "    return input_z"
      ],
      "metadata": {
        "id": "HFMGMXApkLmX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def d_train(x):\n",
        "    disc_model.zero_grad()\n",
        "\n",
        "    #Train discriminator with real batch\n",
        "    batch_size = x.size(0)\n",
        "    x = x.to(device)\n",
        "    d_labels_real = torch.ones(batch_size, 1, device=device)\n",
        "\n",
        "    d_proba_real = disc_model(x)\n",
        "    d_loss_real = loss_fn(d_proba_real, d_labels_real)\n",
        "\n",
        "    #Train discriminator with fake batch\n",
        "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "    g_output = gen_model(input_z)\n",
        "\n",
        "    d_proba_fake = disc_model(g_output)\n",
        "    d_labels_fake = torch.zeros(batch_size, 1, device=device)\n",
        "    d_loss_fake = loss_fn(d_proba_fake, d_labels_fake)\n",
        "\n",
        "    #gradient backpro & optimize ONLY D's parameters\n",
        "    d_loss = d_loss_real + d_loss_fake\n",
        "    d_loss.backward()\n",
        "    d_optimizer.step()\n",
        "\n",
        "    return d_loss.data.item(), d_proba_real.detach(), d_proba_fake.detach()"
      ],
      "metadata": {
        "id": "AgnPs9IKkbTg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train the generator\n",
        "\n",
        "def g_train(x):\n",
        "    gen_model.zero_grad()\n",
        "\n",
        "    batch_size = x.size(0)\n",
        "    input_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "    g_labels_real = torch.ones((batch_size, 1), device=device)\n",
        "\n",
        "    g_output = gen_model(input_z)\n",
        "    d_proba_fake = disc_model(g_output)\n",
        "    g_loss = loss_fn(d_proba_fake, g_labels_real)\n",
        "\n",
        "    #gradient backprop & optimize ONLY G's parameters\n",
        "    g_loss.backward()\n",
        "    g_optimizer.step()\n",
        "\n",
        "    return g_loss.data.item()"
      ],
      "metadata": {
        "id": "QggdnuAdlntw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mode_z = 'uniform'\n",
        "fixed_z = create_noise(batch_size, z_size, mode_z).to(device)\n",
        "\n",
        "def create_samples(g_model, input_z):\n",
        "    g_output = g_model(input_z)\n",
        "    images = torch.reshape(g_output, (batch_size, *image_size))\n",
        "    return (images+1)/2.0\n",
        "\n",
        "epoch_samples = []\n",
        "\n",
        "num_epochs = 50\n",
        "torch.manual_seed(1)\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    gen_model.train()\n",
        "    d_losses, g_losses = [], []\n",
        "    for i, (x, _) in enumerate(mnist_dl):\n",
        "        d_loss, d_proba_real, d_proba_fake = d_train(x)\n",
        "        d_losses.append(d_loss)\n",
        "        g_losses.append(g_train(x))\n",
        "\n",
        "    print(f'Epoch {epoch:03d} | Avg Losses >>'\n",
        "          f' G/D {torch.FloatTensor(g_losses).mean():.4f}'\n",
        "          f'/{torch.FloatTensor(d_losses).mean():.4f}')\n",
        "    gen_model.eval()\n",
        "    epoch_samples.append(\n",
        "        create_samples(gen_model, fixed_z).detach().cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPOz5cjimiPy",
        "outputId": "55d094c9-ab7c-4954-83b0-8bacc47a0404"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | Avg Losses >> G/D 4.7359/0.1070\n",
            "Epoch 002 | Avg Losses >> G/D 4.8516/0.1224\n",
            "Epoch 003 | Avg Losses >> G/D 4.1261/0.2119\n",
            "Epoch 004 | Avg Losses >> G/D 3.4437/0.2885\n",
            "Epoch 005 | Avg Losses >> G/D 3.2799/0.2699\n",
            "Epoch 006 | Avg Losses >> G/D 3.2663/0.2788\n",
            "Epoch 007 | Avg Losses >> G/D 3.1611/0.2872\n",
            "Epoch 008 | Avg Losses >> G/D 3.1597/0.2593\n",
            "Epoch 009 | Avg Losses >> G/D 3.2285/0.2569\n",
            "Epoch 010 | Avg Losses >> G/D 3.2586/0.2685\n",
            "Epoch 011 | Avg Losses >> G/D 3.2922/0.2379\n",
            "Epoch 012 | Avg Losses >> G/D 3.3413/0.2264\n",
            "Epoch 013 | Avg Losses >> G/D 3.3884/0.2309\n",
            "Epoch 014 | Avg Losses >> G/D 3.4510/0.2353\n",
            "Epoch 015 | Avg Losses >> G/D 3.4284/0.2307\n",
            "Epoch 016 | Avg Losses >> G/D 3.4237/0.2179\n",
            "Epoch 017 | Avg Losses >> G/D 3.5536/0.2116\n",
            "Epoch 018 | Avg Losses >> G/D 3.5781/0.2230\n",
            "Epoch 019 | Avg Losses >> G/D 3.6142/0.2158\n",
            "Epoch 020 | Avg Losses >> G/D 3.6359/0.1932\n",
            "Epoch 021 | Avg Losses >> G/D 3.7106/0.2015\n",
            "Epoch 022 | Avg Losses >> G/D 3.7817/0.2042\n",
            "Epoch 023 | Avg Losses >> G/D 3.6691/0.2116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "grdlBWCpmlLn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}